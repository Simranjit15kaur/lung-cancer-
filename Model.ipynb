{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GKDkn4SLy-9",
        "outputId": "cf965c3c-e3c3-454c-ec70-6440f8f14422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training dataset...\n",
            "Loading testing dataset...\n",
            "Label mapping: {'Benign': 0, 'Cancer': 1, 'Control': 2}\n",
            "Training set size: (1281, 1620)\n",
            "Test set size: (427, 1620)\n",
            "Number of classes: 3\n",
            "Using device: cuda\n",
            "Training base models on training dataset...\n",
            "Training XGBoost...\n",
            "XGBoost training completed.\n",
            "Training LightGBM...\n",
            "LightGBM training completed.\n",
            "Training Enhanced Transformer...\n",
            "Epoch 1/50 - Loss: 1.0632 - Train Acc: 0.5498 - Val Acc: 0.7082\n",
            "Epoch 6/50 - Loss: 0.6300 - Train Acc: 0.8105 - Val Acc: 0.7821\n",
            "Epoch 11/50 - Loss: 0.5451 - Train Acc: 0.8789 - Val Acc: 0.7510\n",
            "Epoch 16/50 - Loss: 0.4071 - Train Acc: 0.9531 - Val Acc: 0.7549\n",
            "Early stopping at epoch 18\n",
            "Best transformer validation accuracy: 0.7938\n",
            "Enhanced Transformer training completed.\n",
            "Creating stacking ensemble...\n",
            "Processing fold 1/5\n",
            "Epoch 1/30 - Loss: 1.0865 - Train Acc: 0.5625 - Val Acc: 0.7315\n",
            "Epoch 6/30 - Loss: 0.6199 - Train Acc: 0.8232 - Val Acc: 0.7432\n",
            "Epoch 11/30 - Loss: 0.5525 - Train Acc: 0.8691 - Val Acc: 0.7588\n",
            "Early stopping at epoch 12\n",
            "Best transformer validation accuracy: 0.7743\n",
            "Processing fold 2/5\n",
            "Epoch 1/30 - Loss: 1.1055 - Train Acc: 0.5268 - Val Acc: 0.7070\n",
            "Epoch 6/30 - Loss: 0.6740 - Train Acc: 0.8293 - Val Acc: 0.7891\n",
            "Epoch 11/30 - Loss: 0.5413 - Train Acc: 0.8878 - Val Acc: 0.7773\n",
            "Epoch 16/30 - Loss: 0.3803 - Train Acc: 0.9688 - Val Acc: 0.7656\n",
            "Early stopping at epoch 17\n",
            "Best transformer validation accuracy: 0.7930\n",
            "Processing fold 3/5\n",
            "Epoch 1/30 - Loss: 1.0319 - Train Acc: 0.5580 - Val Acc: 0.7070\n",
            "Epoch 6/30 - Loss: 0.8267 - Train Acc: 0.8205 - Val Acc: 0.7305\n",
            "Epoch 11/30 - Loss: 0.5281 - Train Acc: 0.8771 - Val Acc: 0.7656\n",
            "Epoch 16/30 - Loss: 0.3918 - Train Acc: 0.9629 - Val Acc: 0.7656\n",
            "Early stopping at epoch 20\n",
            "Best transformer validation accuracy: 0.7930\n",
            "Processing fold 4/5\n",
            "Epoch 1/30 - Loss: 1.0367 - Train Acc: 0.5512 - Val Acc: 0.7109\n",
            "Epoch 6/30 - Loss: 0.6344 - Train Acc: 0.8156 - Val Acc: 0.7227\n",
            "Epoch 11/30 - Loss: 0.5348 - Train Acc: 0.8878 - Val Acc: 0.7617\n",
            "Early stopping at epoch 15\n",
            "Best transformer validation accuracy: 0.7773\n",
            "Processing fold 5/5\n",
            "Epoch 1/30 - Loss: 1.0623 - Train Acc: 0.5668 - Val Acc: 0.6953\n",
            "Epoch 6/30 - Loss: 0.6362 - Train Acc: 0.7941 - Val Acc: 0.7695\n",
            "Epoch 11/30 - Loss: 0.5657 - Train Acc: 0.8634 - Val Acc: 0.7734\n",
            "Early stopping at epoch 15\n",
            "Best transformer validation accuracy: 0.7891\n",
            "Stacking ensemble training completed.\n",
            "\n",
            "============================================================\n",
            "EVALUATING ON TEST DATASET\n",
            "============================================================\n",
            "Getting individual model predictions...\n",
            "XGBoost Test - Accuracy: 0.9625, F1: 0.9608\n",
            "LightGBM Test - Accuracy: 0.9719, F1: 0.9709\n",
            "Enhanced Transformer Test - Accuracy: 0.8220, F1: 0.8252\n",
            "Evaluating voting ensemble on test set...\n",
            "Voting Ensemble Test - Accuracy: 0.9672, F1: 0.9659\n",
            "Evaluating weighted ensemble on test set...\n",
            "Weighted Ensemble Test - Accuracy: 0.9672, F1: 0.9659\n",
            "Evaluating stacking ensemble on test set...\n",
            "Stacking Ensemble Test - Accuracy: 0.4988, F1: 0.3819\n",
            "\n",
            "Test Set Results Comparison:\n",
            "     Method  Accuracy  F1-Score\n",
            "    XGBoost  0.962529  0.960797\n",
            "   LightGBM  0.971897  0.970947\n",
            "Transformer  0.822014  0.825181\n",
            "     Voting  0.967213  0.965899\n",
            "   Weighted  0.967213  0.965899\n",
            "   Stacking  0.498829  0.381915\n",
            "\n",
            "Best method on test set: LightGBM (F1: 0.9709)\n",
            "\n",
            "Detailed Classification Report for LightGBM on Test Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.84      0.92        77\n",
            "           1       0.97      1.00      0.98       157\n",
            "           2       0.96      1.00      0.98       193\n",
            "\n",
            "    accuracy                           0.97       427\n",
            "   macro avg       0.98      0.95      0.96       427\n",
            "weighted avg       0.97      0.97      0.97       427\n",
            "\n",
            "\n",
            "Model training completed! You can now use:\n",
            "- hybrid_model.predict(X_new, method='weighted') for new predictions\n",
            "- hybrid_model.predict(X_new, method='voting') for voting ensemble\n",
            "- hybrid_model.predict(X_new, method='stacking') for stacking ensemble\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class ImprovedTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes=3, d_model=512, nhead=8, num_layers=6, dropout=0.2):\n",
        "        super(ImprovedTransformer, self).__init__()\n",
        "\n",
        "        # Input projection\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        self.layer_norm_input = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(1, 1, d_model))\n",
        "\n",
        "        # Transformer layers with residual connections\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=d_model*2,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True  # Pre-norm architecture\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Classification head with multiple layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model // 2, d_model // 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model // 4, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input projection and normalization\n",
        "        x = self.input_projection(x)  # (batch_size, input_dim) -> (batch_size, d_model)\n",
        "        x = self.layer_norm_input(x)\n",
        "\n",
        "        # Add positional encoding and reshape for transformer\n",
        "        x = x.unsqueeze(1) + self.pos_encoding  # (batch_size, 1, d_model)\n",
        "\n",
        "        # Transformer encoding\n",
        "        x = self.transformer(x)  # (batch_size, 1, d_model)\n",
        "\n",
        "        # Classification\n",
        "        x = x.squeeze(1)  # (batch_size, d_model)\n",
        "        return self.classifier(x)\n",
        "\n",
        "class HybridEnsemble:\n",
        "    def __init__(self, input_dim, num_classes=3, device='cpu'):\n",
        "        self.input_dim = input_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.meta_model = None\n",
        "        self.label_mapping = None\n",
        "\n",
        "    def create_xgboost_model(self):\n",
        "        \"\"\"Create and return XGBoost model with optimized parameters\"\"\"\n",
        "        return xgb.XGBClassifier(\n",
        "            n_estimators=200,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.9,\n",
        "            colsample_bytree=0.9,\n",
        "            random_state=42,\n",
        "            eval_metric='mlogloss',\n",
        "            tree_method='hist'\n",
        "        )\n",
        "\n",
        "    def create_lightgbm_model(self):\n",
        "        \"\"\"Create and return LightGBM model with optimized parameters\"\"\"\n",
        "        return lgb.LGBMClassifier(\n",
        "            n_estimators=200,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.9,\n",
        "            colsample_bytree=0.9,\n",
        "            random_state=42,\n",
        "            verbose=-1,\n",
        "            force_col_wise=True\n",
        "        )\n",
        "\n",
        "    def create_transformer_model(self):\n",
        "        \"\"\"Create and return improved transformer model\"\"\"\n",
        "        model = ImprovedTransformer(\n",
        "            input_dim=self.input_dim,\n",
        "            num_classes=self.num_classes,\n",
        "            d_model=256,  # Reduced for better generalization\n",
        "            nhead=8,\n",
        "            num_layers=4,  # Reduced to prevent overfitting\n",
        "            dropout=0.3\n",
        "        ).to(self.device)\n",
        "        return model\n",
        "\n",
        "    def train_transformer_with_tuning(self, X_train, y_train, X_val=None, y_val=None, epochs=50):\n",
        "        \"\"\"Train transformer with improved techniques\"\"\"\n",
        "        model = self.create_transformer_model()\n",
        "\n",
        "        # Loss function with label smoothing\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        # AdamW optimizer with weight decay\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "        # Cosine annealing scheduler\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "        # Convert to tensors\n",
        "        X_train_tensor = torch.FloatTensor(X_train).to(self.device)\n",
        "        y_train_tensor = torch.LongTensor(y_train).to(self.device)\n",
        "\n",
        "        # Create validation tensors if provided\n",
        "        if X_val is not None and y_val is not None:\n",
        "            X_val_tensor = torch.FloatTensor(X_val).to(self.device)\n",
        "            y_val_tensor = torch.LongTensor(y_val).to(self.device)\n",
        "\n",
        "        # Create data loaders\n",
        "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        best_val_acc = 0\n",
        "        best_model_state = None\n",
        "        patience = 10\n",
        "        patience_counter = 0\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            correct_train = 0\n",
        "            total_train = 0\n",
        "\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_train += batch_y.size(0)\n",
        "                correct_train += (predicted == batch_y).sum().item()\n",
        "\n",
        "            # Evaluation\n",
        "            train_acc = correct_train / total_train\n",
        "            avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "            # Validation evaluation if validation data is provided\n",
        "            if X_val is not None and y_val is not None:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_outputs = model(X_val_tensor)\n",
        "                    _, val_predicted = torch.max(val_outputs.data, 1)\n",
        "                    val_acc = (val_predicted == y_val_tensor).sum().item() / y_val_tensor.size(0)\n",
        "\n",
        "                if val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    best_model_state = model.state_dict().copy()\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "\n",
        "                if epoch % 5 == 0:\n",
        "                    print(f'Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Train Acc: {train_acc:.4f} - Val Acc: {val_acc:.4f}')\n",
        "\n",
        "                if patience_counter >= patience:\n",
        "                    print(f'Early stopping at epoch {epoch+1}')\n",
        "                    break\n",
        "\n",
        "                model.train()\n",
        "            else:\n",
        "                # No validation data, just print training progress\n",
        "                if epoch % 5 == 0:\n",
        "                    print(f'Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Train Acc: {train_acc:.4f}')\n",
        "\n",
        "        # Load best model if validation was used\n",
        "        if best_model_state is not None:\n",
        "            model.load_state_dict(best_model_state)\n",
        "            print(f'Best transformer validation accuracy: {best_val_acc:.4f}')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_base_models(self, X_train, y_train):\n",
        "        \"\"\"Train all base models using only training data\"\"\"\n",
        "        print(\"Training base models on training dataset...\")\n",
        "\n",
        "        # Create validation split from training data for transformer validation\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        "        )\n",
        "\n",
        "        # Scale features for transformer\n",
        "        scaler_transformer = StandardScaler()\n",
        "        X_train_scaled = scaler_transformer.fit_transform(X_train_split)\n",
        "        X_val_scaled = scaler_transformer.transform(X_val_split)\n",
        "        self.scalers['transformer'] = scaler_transformer\n",
        "\n",
        "        # Train XGBoost\n",
        "        print(\"Training XGBoost...\")\n",
        "        xgb_model = self.create_xgboost_model()\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "        self.models['xgboost'] = xgb_model\n",
        "        print(\"XGBoost training completed.\")\n",
        "\n",
        "        # Train LightGBM\n",
        "        print(\"Training LightGBM...\")\n",
        "        lgb_model = self.create_lightgbm_model()\n",
        "        lgb_model.fit(X_train, y_train)\n",
        "        self.models['lightgbm'] = lgb_model\n",
        "        print(\"LightGBM training completed.\")\n",
        "\n",
        "        # Train Transformer\n",
        "        print(\"Training Enhanced Transformer...\")\n",
        "        transformer_model = self.train_transformer_with_tuning(\n",
        "            X_train_scaled, y_train_split, X_val_scaled, y_val_split, epochs=50\n",
        "        )\n",
        "        self.models['transformer'] = transformer_model\n",
        "        print(\"Enhanced Transformer training completed.\")\n",
        "\n",
        "    def create_stacking_ensemble(self, X_train, y_train):\n",
        "        \"\"\"Create stacking ensemble using meta-learner with cross-validation on training data\"\"\"\n",
        "        print(\"Creating stacking ensemble...\")\n",
        "\n",
        "        # Generate base model predictions using cross-validation on training data\n",
        "        cv_folds = 5\n",
        "        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "        # Initialize meta-features\n",
        "        meta_features_train = np.zeros((len(X_train), len(self.models)))\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
        "            print(f\"Processing fold {fold + 1}/{cv_folds}\")\n",
        "\n",
        "            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
        "            y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "            # XGBoost predictions\n",
        "            fold_xgb = self.create_xgboost_model()\n",
        "            fold_xgb.fit(X_fold_train, y_fold_train)\n",
        "            meta_features_train[val_idx, 0] = fold_xgb.predict_proba(X_fold_val)[:, 1] if len(np.unique(y_train)) == 2 else fold_xgb.predict_proba(X_fold_val).max(axis=1)\n",
        "\n",
        "            # LightGBM predictions\n",
        "            fold_lgb = self.create_lightgbm_model()\n",
        "            fold_lgb.fit(X_fold_train, y_fold_train)\n",
        "            meta_features_train[val_idx, 1] = fold_lgb.predict_proba(X_fold_val)[:, 1] if len(np.unique(y_train)) == 2 else fold_lgb.predict_proba(X_fold_val).max(axis=1)\n",
        "\n",
        "            # Transformer predictions\n",
        "            scaler_fold = StandardScaler()\n",
        "            X_fold_train_scaled = scaler_fold.fit_transform(X_fold_train)\n",
        "            X_fold_val_scaled = scaler_fold.transform(X_fold_val)\n",
        "\n",
        "            fold_transformer = self.train_transformer_with_tuning(\n",
        "                X_fold_train_scaled, y_fold_train, X_fold_val_scaled, y_fold_val, epochs=30\n",
        "            )\n",
        "            fold_transformer.eval()\n",
        "            with torch.no_grad():\n",
        "                X_val_tensor = torch.FloatTensor(X_fold_val_scaled).to(self.device)\n",
        "                val_outputs = fold_transformer(X_val_tensor)\n",
        "                val_probs = torch.softmax(val_outputs, dim=1).cpu().numpy()\n",
        "                meta_features_train[val_idx, 2] = val_probs.max(axis=1)\n",
        "\n",
        "        # Train meta-learner\n",
        "        self.meta_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "        self.meta_model.fit(meta_features_train, y_train)\n",
        "\n",
        "        print(\"Stacking ensemble training completed.\")\n",
        "\n",
        "    def evaluate_on_test_set(self, X_test, y_test):\n",
        "        \"\"\"Evaluate all ensemble methods on the test dataset\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"EVALUATING ON TEST DATASET\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Prepare test data for transformer\n",
        "        X_test_scaled = self.scalers['transformer'].transform(X_test)\n",
        "\n",
        "        # Get individual model predictions on test set\n",
        "        print(\"Getting individual model predictions...\")\n",
        "        xgb_pred = self.models['xgboost'].predict(X_test)\n",
        "        xgb_acc = accuracy_score(y_test, xgb_pred)\n",
        "        xgb_f1 = f1_score(y_test, xgb_pred, average='weighted')\n",
        "        print(f\"XGBoost Test - Accuracy: {xgb_acc:.4f}, F1: {xgb_f1:.4f}\")\n",
        "\n",
        "        lgb_pred = self.models['lightgbm'].predict(X_test)\n",
        "        lgb_acc = accuracy_score(y_test, lgb_pred)\n",
        "        lgb_f1 = f1_score(y_test, lgb_pred, average='weighted')\n",
        "        print(f\"LightGBM Test - Accuracy: {lgb_acc:.4f}, F1: {lgb_f1:.4f}\")\n",
        "\n",
        "        # Transformer predictions on test set\n",
        "        self.models['transformer'].eval()\n",
        "        with torch.no_grad():\n",
        "            X_test_tensor = torch.FloatTensor(X_test_scaled).to(self.device)\n",
        "            transformer_outputs = self.models['transformer'](X_test_tensor)\n",
        "            _, transformer_pred = torch.max(transformer_outputs, 1)\n",
        "            transformer_pred = transformer_pred.cpu().numpy()\n",
        "\n",
        "        transformer_acc = accuracy_score(y_test, transformer_pred)\n",
        "        transformer_f1 = f1_score(y_test, transformer_pred, average='weighted')\n",
        "        print(f\"Enhanced Transformer Test - Accuracy: {transformer_acc:.4f}, F1: {transformer_f1:.4f}\")\n",
        "\n",
        "        # Voting ensemble on test set\n",
        "        voting_pred, voting_acc, voting_f1 = self.create_voting_ensemble(X_test, y_test)\n",
        "\n",
        "        # Weighted ensemble on test set\n",
        "        weighted_pred, weighted_acc, weighted_f1 = self.create_weighted_ensemble(X_test, y_test)\n",
        "\n",
        "        # Stacking ensemble on test set\n",
        "        stacking_pred, stacking_acc, stacking_f1 = self.evaluate_stacking_on_test(X_test, y_test)\n",
        "\n",
        "        # Compare results\n",
        "        results = {\n",
        "            'Method': ['XGBoost', 'LightGBM', 'Transformer', 'Voting', 'Weighted', 'Stacking'],\n",
        "            'Accuracy': [xgb_acc, lgb_acc, transformer_acc, voting_acc, weighted_acc, stacking_acc],\n",
        "            'F1-Score': [xgb_f1, lgb_f1, transformer_f1, voting_f1, weighted_f1, stacking_f1]\n",
        "        }\n",
        "\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\nTest Set Results Comparison:\")\n",
        "        print(results_df.to_string(index=False))\n",
        "\n",
        "        # Find best method\n",
        "        best_method_idx = np.argmax(results_df['F1-Score'])\n",
        "        best_method = results_df.iloc[best_method_idx]['Method']\n",
        "        best_f1 = results_df.iloc[best_method_idx]['F1-Score']\n",
        "\n",
        "        print(f\"\\nBest method on test set: {best_method} (F1: {best_f1:.4f})\")\n",
        "\n",
        "        # Detailed classification report for best method\n",
        "        if best_method == 'Voting':\n",
        "            best_pred = voting_pred\n",
        "        elif best_method == 'Weighted':\n",
        "            best_pred = weighted_pred\n",
        "        elif best_method == 'Stacking':\n",
        "            best_pred = stacking_pred\n",
        "        elif best_method == 'XGBoost':\n",
        "            best_pred = xgb_pred\n",
        "        elif best_method == 'LightGBM':\n",
        "            best_pred = lgb_pred\n",
        "        else:\n",
        "            best_pred = transformer_pred\n",
        "\n",
        "        print(f\"\\nDetailed Classification Report for {best_method} on Test Set:\")\n",
        "        print(classification_report(y_test, best_pred))\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def evaluate_stacking_on_test(self, X_test, y_test):\n",
        "        \"\"\"Evaluate stacking ensemble on test set\"\"\"\n",
        "        print(\"Evaluating stacking ensemble on test set...\")\n",
        "\n",
        "        X_test_scaled = self.scalers['transformer'].transform(X_test)\n",
        "\n",
        "        # Generate test meta-features using trained base models\n",
        "        meta_features_test = np.zeros((len(X_test), len(self.models)))\n",
        "\n",
        "        # XGBoost test predictions\n",
        "        xgb_test_probs = self.models['xgboost'].predict_proba(X_test)\n",
        "        meta_features_test[:, 0] = xgb_test_probs[:, 1] if xgb_test_probs.shape[1] == 2 else xgb_test_probs.max(axis=1)\n",
        "\n",
        "        # LightGBM test predictions\n",
        "        lgb_test_probs = self.models['lightgbm'].predict_proba(X_test)\n",
        "        meta_features_test[:, 1] = lgb_test_probs[:, 1] if lgb_test_probs.shape[1] == 2 else lgb_test_probs.max(axis=1)\n",
        "\n",
        "        # Transformer test predictions\n",
        "        self.models['transformer'].eval()\n",
        "        with torch.no_grad():\n",
        "            X_test_tensor = torch.FloatTensor(X_test_scaled).to(self.device)\n",
        "            test_outputs = self.models['transformer'](X_test_tensor)\n",
        "            test_probs = torch.softmax(test_outputs, dim=1).cpu().numpy()\n",
        "            meta_features_test[:, 2] = test_probs.max(axis=1)\n",
        "\n",
        "        # Use meta-learner to make final predictions\n",
        "        stacking_pred = self.meta_model.predict(meta_features_test)\n",
        "        stacking_acc = accuracy_score(y_test, stacking_pred)\n",
        "        stacking_f1 = f1_score(y_test, stacking_pred, average='weighted')\n",
        "\n",
        "        print(f\"Stacking Ensemble Test - Accuracy: {stacking_acc:.4f}, F1: {stacking_f1:.4f}\")\n",
        "\n",
        "        return stacking_pred, stacking_acc, stacking_f1\n",
        "\n",
        "    def create_voting_ensemble(self, X_test, y_test):\n",
        "        \"\"\"Create voting ensemble on test set\"\"\"\n",
        "        print(\"Evaluating voting ensemble on test set...\")\n",
        "\n",
        "        X_test_scaled = self.scalers['transformer'].transform(X_test)\n",
        "\n",
        "        # Get predictions from all models\n",
        "        xgb_pred = self.models['xgboost'].predict(X_test)\n",
        "        lgb_pred = self.models['lightgbm'].predict(X_test)\n",
        "\n",
        "        # Transformer predictions\n",
        "        self.models['transformer'].eval()\n",
        "        with torch.no_grad():\n",
        "            X_test_tensor = torch.FloatTensor(X_test_scaled).to(self.device)\n",
        "            transformer_outputs = self.models['transformer'](X_test_tensor)\n",
        "            _, transformer_pred = torch.max(transformer_outputs, 1)\n",
        "            transformer_pred = transformer_pred.cpu().numpy()\n",
        "\n",
        "        # Majority voting\n",
        "        predictions = np.array([xgb_pred, lgb_pred, transformer_pred])\n",
        "        voting_pred = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
        "\n",
        "        voting_acc = accuracy_score(y_test, voting_pred)\n",
        "        voting_f1 = f1_score(y_test, voting_pred, average='weighted')\n",
        "\n",
        "        print(f\"Voting Ensemble Test - Accuracy: {voting_acc:.4f}, F1: {voting_f1:.4f}\")\n",
        "\n",
        "        return voting_pred, voting_acc, voting_f1\n",
        "\n",
        "    def create_weighted_ensemble(self, X_test, y_test):\n",
        "        \"\"\"Create weighted ensemble on test set\"\"\"\n",
        "        print(\"Evaluating weighted ensemble on test set...\")\n",
        "\n",
        "        X_test_scaled = self.scalers['transformer'].transform(X_test)\n",
        "\n",
        "        # Get probability predictions from all models\n",
        "        xgb_probs = self.models['xgboost'].predict_proba(X_test)\n",
        "        lgb_probs = self.models['lightgbm'].predict_proba(X_test)\n",
        "\n",
        "        # Transformer probabilities\n",
        "        self.models['transformer'].eval()\n",
        "        with torch.no_grad():\n",
        "            X_test_tensor = torch.FloatTensor(X_test_scaled).to(self.device)\n",
        "            transformer_outputs = self.models['transformer'](X_test_tensor)\n",
        "            transformer_probs = torch.softmax(transformer_outputs, dim=1).cpu().numpy()\n",
        "\n",
        "        # Weights based on expected performance (you can adjust these)\n",
        "        weights = [0.3, 0.4, 0.3]  # Equal weights initially, can be tuned\n",
        "\n",
        "        # Weighted average\n",
        "        weighted_probs = (weights[0] * xgb_probs +\n",
        "                         weights[1] * lgb_probs +\n",
        "                         weights[2] * transformer_probs)\n",
        "\n",
        "        weighted_pred = np.argmax(weighted_probs, axis=1)\n",
        "\n",
        "        weighted_acc = accuracy_score(y_test, weighted_pred)\n",
        "        weighted_f1 = f1_score(y_test, weighted_pred, average='weighted')\n",
        "\n",
        "        print(f\"Weighted Ensemble Test - Accuracy: {weighted_acc:.4f}, F1: {weighted_f1:.4f}\")\n",
        "\n",
        "        return weighted_pred, weighted_acc, weighted_f1\n",
        "\n",
        "    def predict(self, X, method='weighted'):\n",
        "        \"\"\"Make predictions using specified ensemble method\"\"\"\n",
        "        if method == 'stacking' and self.meta_model is None:\n",
        "            raise ValueError(\"Stacking model not trained. Use fit method first.\")\n",
        "\n",
        "        X_scaled = self.scalers['transformer'].transform(X)\n",
        "\n",
        "        if method == 'voting':\n",
        "            # Get predictions from all models\n",
        "            xgb_pred = self.models['xgboost'].predict(X)\n",
        "            lgb_pred = self.models['lightgbm'].predict(X)\n",
        "\n",
        "            self.models['transformer'].eval()\n",
        "            with torch.no_grad():\n",
        "                X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
        "                transformer_outputs = self.models['transformer'](X_tensor)\n",
        "                _, transformer_pred = torch.max(transformer_outputs, 1)\n",
        "                transformer_pred = transformer_pred.cpu().numpy()\n",
        "\n",
        "            predictions = np.array([xgb_pred, lgb_pred, transformer_pred])\n",
        "            return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
        "\n",
        "        elif method == 'weighted':\n",
        "            xgb_probs = self.models['xgboost'].predict_proba(X)\n",
        "            lgb_probs = self.models['lightgbm'].predict_proba(X)\n",
        "\n",
        "            self.models['transformer'].eval()\n",
        "            with torch.no_grad():\n",
        "                X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
        "                transformer_outputs = self.models['transformer'](X_tensor)\n",
        "                transformer_probs = torch.softmax(transformer_outputs, dim=1).cpu().numpy()\n",
        "\n",
        "            weights = [0.3, 0.4, 0.3]\n",
        "            weighted_probs = (weights[0] * xgb_probs +\n",
        "                             weights[1] * lgb_probs +\n",
        "                             weights[2] * transformer_probs)\n",
        "\n",
        "            return np.argmax(weighted_probs, axis=1)\n",
        "\n",
        "        elif method == 'stacking':\n",
        "            # Generate meta-features\n",
        "            meta_features = np.zeros((len(X), len(self.models)))\n",
        "\n",
        "            xgb_probs = self.models['xgboost'].predict_proba(X)\n",
        "            meta_features[:, 0] = xgb_probs[:, 1] if xgb_probs.shape[1] == 2 else xgb_probs.max(axis=1)\n",
        "\n",
        "            lgb_probs = self.models['lightgbm'].predict_proba(X)\n",
        "            meta_features[:, 1] = lgb_probs[:, 1] if lgb_probs.shape[1] == 2 else lgb_probs.max(axis=1)\n",
        "\n",
        "            self.models['transformer'].eval()\n",
        "            with torch.no_grad():\n",
        "                X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
        "                transformer_outputs = self.models['transformer'](X_tensor)\n",
        "                transformer_probs = torch.softmax(transformer_outputs, dim=1).cpu().numpy()\n",
        "                meta_features[:, 2] = transformer_probs.max(axis=1)\n",
        "\n",
        "            return self.meta_model.predict(meta_features)\n",
        "\n",
        "def main():\n",
        "    # Load your datasets\n",
        "    print(\"Loading training dataset...\")\n",
        "    train_df = pd.read_csv('train_set_fu.csv')  # Replace with your training file path\n",
        "\n",
        "    print(\"Loading testing dataset...\")\n",
        "    test_df = pd.read_csv('test_set_fu.csv')   # Replace with your testing file path\n",
        "\n",
        "    # Prepare training data\n",
        "    X_train = train_df.drop('label', axis=1).values\n",
        "    y_train = train_df['label'].values\n",
        "\n",
        "    # Prepare testing data\n",
        "    X_test = test_df.drop('label', axis=1).values\n",
        "    y_test = test_df['label'].values\n",
        "\n",
        "    # Convert labels to numeric if they're strings\n",
        "    if y_train.dtype == 'object':\n",
        "        unique_labels = np.unique(np.concatenate([y_train, y_test]))\n",
        "        label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "        y_train = np.array([label_mapping[label] for label in y_train])\n",
        "        y_test = np.array([label_mapping[label] for label in y_test])\n",
        "        print(f\"Label mapping: {label_mapping}\")\n",
        "\n",
        "    print(f\"Training set size: {X_train.shape}\")\n",
        "    print(f\"Test set size: {X_test.shape}\")\n",
        "    print(f\"Number of classes: {len(np.unique(np.concatenate([y_train, y_test])))}\")\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create hybrid ensemble\n",
        "    hybrid_model = HybridEnsemble(\n",
        "        input_dim=X_train.shape[1],\n",
        "        num_classes=len(np.unique(np.concatenate([y_train, y_test]))),\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Train base models on training data only\n",
        "    hybrid_model.train_base_models(X_train, y_train)\n",
        "\n",
        "    # Train stacking ensemble on training data only\n",
        "    hybrid_model.create_stacking_ensemble(X_train, y_train)\n",
        "\n",
        "    # Evaluate on test dataset\n",
        "    results_df = hybrid_model.evaluate_on_test_set(X_test, y_test)\n",
        "\n",
        "    return hybrid_model, results_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    hybrid_model, results = main()\n",
        "\n",
        "    # Example of how to use the trained model for new predictions\n",
        "    print(\"\\nModel training completed! You can now use:\")\n",
        "    print(\"- hybrid_model.predict(X_new, method='weighted') for new predictions\")\n",
        "    print(\"- hybrid_model.predict(X_new, method='voting') for voting ensemble\")\n",
        "    print(\"- hybrid_model.predict(X_new, method='stacking') for stacking ensemble\")"
      ]
    }
  ]
}